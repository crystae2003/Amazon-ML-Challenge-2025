{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823a51aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T13:46:36.131129Z",
     "iopub.status.busy": "2025-10-13T13:46:36.130825Z",
     "iopub.status.idle": "2025-10-13T13:46:48.694453Z",
     "shell.execute_reply": "2025-10-13T13:46:48.693667Z"
    },
    "papermill": {
     "duration": 12.568125,
     "end_time": "2025-10-13T13:46:48.695815",
     "exception": false,
     "start_time": "2025-10-13T13:46:36.127690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.30.2\r\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (3.19.1)\r\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2)\r\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2025.9.18)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.32.5)\r\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\r\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2025.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (1.1.10)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2025.8.3)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.30.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.30.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.30.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.30.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.30.2) (2024.2.0)\r\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tokenizers, huggingface-hub, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.21.2\r\n",
      "    Uninstalling tokenizers-0.21.2:\r\n",
      "      Successfully uninstalled tokenizers-0.21.2\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 1.0.0rc2\r\n",
      "    Uninstalling huggingface-hub-1.0.0rc2:\r\n",
      "      Successfully uninstalled huggingface-hub-1.0.0rc2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.53.3\r\n",
      "    Uninstalling transformers-4.53.3:\r\n",
      "      Successfully uninstalled transformers-4.53.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "kaggle-environments 1.18.0 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\r\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.35.3 tokenizers-0.13.3 transformers-4.30.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f09fb7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T13:46:48.703207Z",
     "iopub.status.busy": "2025-10-13T13:46:48.702878Z",
     "iopub.status.idle": "2025-10-13T13:46:48.713750Z",
     "shell.execute_reply": "2025-10-13T13:46:48.713065Z"
    },
    "papermill": {
     "duration": 0.015733,
     "end_time": "2025-10-13T13:46:48.714788",
     "exception": false,
     "start_time": "2025-10-13T13:46:48.699055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ddp_price_prediction_final_stable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_price_prediction_final_stable.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# DDP Imports\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# Mixed Precision Imports\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ============== 1. CONFIGURATION ==============\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 90\n",
    "LEARNING_RATE = 2e-5\n",
    "SAVE_DIR = './saved_models'\n",
    "EMBEDDINGS_PATH = '/kaggle/input/embeddings/embeddings/embeddings_images.pkl'\n",
    "ID_TO_IGNORE = '279285'\n",
    "PRETRAINED_MODEL_PATH = \"/kaggle/input/amazon-ml-dataset-csv/best_model_44_48.pth\"\n",
    "CHECKPOINT_PATH = os.path.join(SAVE_DIR, 'best_model.pth')\n",
    "\n",
    "\n",
    "# ============== 2. DDP SETUP ==============\n",
    "def ddp_setup():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "\n",
    "# ============== 3. PREPROCESSING ==============\n",
    "def extract_value(text):\n",
    "    match = re.search(r\"Value:\\s*(\\d+\\.?\\d*)\", str(text))\n",
    "    return float(match.group(1)) if match else 0.0\n",
    "\n",
    "def load_embeddings(path):\n",
    "    if not os.path.exists(path): return {}\n",
    "    try:\n",
    "        with open(path, 'rb') as f: return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings file: {e}.\")\n",
    "        return {}\n",
    "        \n",
    "# ============== 4. PYTORCH DATASET ==============\n",
    "class ProductTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length, embeddings_dict, is_test=False):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        self.is_test = is_test\n",
    "        self.numeric_cols = ['extracted_value']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text, sample_id = str(row['catalog_content']), str(row['sample_id'])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_length,\n",
    "            padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        numeric_values = row[self.numeric_cols].values.astype(np.float32)\n",
    "        numeric_features = torch.tensor(numeric_values, dtype=torch.float32)\n",
    "        \n",
    "        embedding_data = self.embeddings_dict.get(sample_id, torch.ones(768, dtype=torch.float32))\n",
    "        if not isinstance(embedding_data, torch.Tensor):\n",
    "            image_embedding = torch.tensor(embedding_data, dtype=torch.float32)\n",
    "        else:\n",
    "            image_embedding = embedding_data\n",
    "            \n",
    "        item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0), 'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'numeric_features': numeric_features, 'image_embedding': image_embedding\n",
    "        }\n",
    "        if self.is_test:\n",
    "            item['sample_id'] = row['sample_id']\n",
    "        else:\n",
    "            item['price'] = torch.tensor(row['price'], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# ============== 5. MODEL ARCHITECTURE ==============\n",
    "class TextPlusNumericModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(TextPlusNumericModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        combined_size = 768 + 1 + 768\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(combined_size, 1024), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numeric_features, image_embedding):\n",
    "        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        combined_features = torch.cat([text_features, numeric_features, image_embedding], dim=1)\n",
    "        price = self.regression_head(combined_features)\n",
    "        return price\n",
    "\n",
    "def smape_loss(y_pred, y_true, epsilon=1e-8):\n",
    "    numerator = torch.abs(y_pred - y_true)\n",
    "    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n",
    "    return torch.mean(numerator / (denominator + epsilon)) * 100\n",
    "\n",
    "# ============== 6. MAIN TRAINING FUNCTION ==============\n",
    "def main_training_and_prediction():\n",
    "    ddp_setup()\n",
    "    rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "    best_val_smape = float('inf')\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"=\"*60 + \"\\nStarting Training...\\n\" + \"=\"*60)\n",
    "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    train_df = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/splits/splits/train.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/splits/splits/val.csv')\n",
    "    train_df = train_df[train_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n",
    "    val_df = val_df[val_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n",
    "    if rank == 0: print(f\"Removed sample ID '{ID_TO_IGNORE}'.\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_df['extracted_value'] = train_df['catalog_content'].apply(extract_value)\n",
    "    val_df['extracted_value'] = val_df['catalog_content'].apply(extract_value)\n",
    "    scaler.fit(train_df[['extracted_value']])\n",
    "    train_df['extracted_value'] = scaler.transform(train_df[['extracted_value']])\n",
    "    val_df['extracted_value'] = scaler.transform(val_df[['extracted_value']])\n",
    "    train_df['price'] = np.log1p(train_df['price'])\n",
    "    val_df['price'] = np.log1p(val_df['price'])\n",
    "\n",
    "    embeddings_dict = load_embeddings(EMBEDDINGS_PATH)\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    train_dataset = ProductTextDataset(train_df, tokenizer, MAX_LENGTH, embeddings_dict)\n",
    "    val_dataset = ProductTextDataset(val_df, tokenizer, MAX_LENGTH, embeddings_dict)\n",
    "    train_sampler, val_sampler = DistributedSampler(train_dataset), DistributedSampler(val_dataset, shuffle=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "    model = TextPlusNumericModel().to(device)\n",
    "    if PRETRAINED_MODEL_PATH and os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        if rank == 0: print(f\"--- Loading pretrained model from: {PRETRAINED_MODEL_PATH} ---\")\n",
    "        model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))\n",
    "    else:\n",
    "        if rank == 0: print(\"--- Starting from scratch. ---\")\n",
    "\n",
    "    model = DDP(model, device_ids=[rank]) \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # ✅ FIX: Reverted to the original API call that works in your environment\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(0.1 * len(train_loader) * EPOCHS), \n",
    "        num_training_steps=len(train_loader) * EPOCHS\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        train_loop = tqdm(train_loader, leave=False, disable=(rank != 0))\n",
    "        if rank == 0: train_loop.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for batch in train_loop:\n",
    "            input_ids, attention_mask, numerics, img_embeds, prices = (\n",
    "                batch['input_ids'].to(device), batch['attention_mask'].to(device),\n",
    "                batch['numeric_features'].to(device), batch['image_embedding'].to(device),\n",
    "                batch['price'].to(device)\n",
    "            )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # ✅ FIX: Reverted to the original API call\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask, numerics, img_embeds)\n",
    "                loss = loss_fn(outputs.squeeze(), prices)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            if rank == 0: train_loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_smape = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # ✅ FIX: Reverted to the original API call\n",
    "                with autocast():\n",
    "                    log_prices_pred = model(\n",
    "                        input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device),\n",
    "                        numeric_features=batch['numeric_features'].to(device), image_embedding=batch['image_embedding'].to(device)\n",
    "                    )\n",
    "                prices_pred_orig = torch.expm1(log_prices_pred.squeeze())\n",
    "                prices_true_orig = torch.expm1(batch['price'].to(device))\n",
    "                total_val_smape += smape_loss(prices_pred_orig, prices_true_orig).item()\n",
    "        \n",
    "        val_metrics_tensor = torch.tensor([total_val_smape, len(val_loader)]).to(device)\n",
    "        dist.all_reduce(val_metrics_tensor, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        if rank == 0:\n",
    "            avg_val_smape = val_metrics_tensor[0] / val_metrics_tensor[1]\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} -> Avg Validation SMAPE: {avg_val_smape:.4f}%\")\n",
    "            if avg_val_smape < best_val_smape:\n",
    "                best_val_smape = avg_val_smape\n",
    "                print(f\"** New best model found! Saving to {CHECKPOINT_PATH} **\")\n",
    "                torch.save(model.module.state_dict(), CHECKPOINT_PATH)\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# ============== 7. LAUNCHER & PREDICTION ==============\n",
    "if __name__ == \"__main__\":\n",
    "    main_training_and_prediction()\n",
    "\n",
    "    if int(os.environ.get(\"RANK\", \"0\")) == 0:\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\nTraining finished. Starting prediction...\\n\" + \"=\"*60)\n",
    "\n",
    "        if os.path.exists(CHECKPOINT_PATH):\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            final_model = TextPlusNumericModel()\n",
    "            final_model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "            final_model.to(device)\n",
    "            final_model.eval()\n",
    "\n",
    "            test_df = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/dataset/dataset/test.csv')\n",
    "            train_df_for_scaler = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/splits/splits/train.csv')\n",
    "            test_df = test_df[test_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n",
    "            \n",
    "            scaler = MinMaxScaler()\n",
    "            train_df_for_scaler['extracted_value'] = train_df_for_scaler['catalog_content'].apply(extract_value)\n",
    "            test_df['extracted_value'] = test_df['catalog_content'].apply(extract_value)\n",
    "            scaler.fit(train_df_for_scaler[['extracted_value']])\n",
    "            test_df['extracted_value'] = scaler.transform(test_df[['extracted_value']])\n",
    "            \n",
    "            tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "            embeddings_dict = load_embeddings(EMBEDDINGS_PATH)\n",
    "            test_dataset = ProductTextDataset(test_df, tokenizer, MAX_LENGTH, embeddings_dict, is_test=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2)\n",
    "\n",
    "            all_predictions, all_sample_ids = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "                    # ✅ FIX: Reverted to the original API call\n",
    "                    with autocast():\n",
    "                        outputs = final_model(\n",
    "                            input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device), \n",
    "                            numeric_features=batch['numeric_features'].to(device), image_embedding=batch['image_embedding'].to(device)\n",
    "                        )\n",
    "                    preds = np.expm1(outputs.squeeze().cpu().numpy())\n",
    "                    all_predictions.extend(preds if preds.ndim > 0 else [preds])\n",
    "                    all_sample_ids.extend(batch['sample_id'])\n",
    "\n",
    "            submission_df = pd.DataFrame({'sample_id': all_sample_ids, 'price': all_predictions})\n",
    "            submission_df.to_csv('output.csv', index=False)\n",
    "            print(\"\\nPredictions saved to output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16096e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T13:46:48.720964Z",
     "iopub.status.busy": "2025-10-13T13:46:48.720755Z",
     "iopub.status.idle": "2025-10-13T17:22:57.356638Z",
     "shell.execute_reply": "2025-10-13T17:22:57.355825Z"
    },
    "papermill": {
     "duration": 12968.640451,
     "end_time": "2025-10-13T17:22:57.358000",
     "exception": false,
     "start_time": "2025-10-13T13:46:48.717549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1013 13:46:52.464000 48 torch/distributed/run.py:792] \r\n",
      "W1013 13:46:52.464000 48 torch/distributed/run.py:792] *****************************************\r\n",
      "W1013 13:46:52.464000 48 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1013 13:46:52.464000 48 torch/distributed/run.py:792] *****************************************\r\n",
      "2025-10-13 13:47:02.168977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-10-13 13:47:02.168970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760363222.331404      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760363222.331391      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760363222.382612      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1760363222.382631      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "============================================================\r\n",
      "Starting Training...\r\n",
      "============================================================\r\n",
      "Removed sample ID '279285'.\r\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 6.87MB/s]\r\n",
      "tokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 301kB/s]\r\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 3.43MB/s]\r\n",
      "model.safetensors: 100%|██████████████████████| 440M/440M [00:01<00:00, 282MB/s]\r\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\r\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\r\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "--- Loading pretrained model from: /kaggle/input/amazon-ml-dataset-csv/best_model_44_48.pth ---\r\n",
      "/kaggle/working/ddp_price_prediction_final_stable.py:161: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = GradScaler()\r\n",
      "/kaggle/working/ddp_price_prediction_final_stable.py:161: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = GradScaler()\r\n",
      "Epoch 1/90:   0%|                                       | 0/250 [00:00<?, ?it/s]/kaggle/working/ddp_price_prediction_final_stable.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with autocast():\r\n",
      "/kaggle/working/ddp_price_prediction_final_stable.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with autocast():\r\n",
      "/kaggle/working/ddp_price_prediction_final_stable.py:198: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with autocast():\r\n",
      "/kaggle/working/ddp_price_prediction_final_stable.py:198: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with autocast():\r\n",
      "Epoch 1/90 -> Avg Validation SMAPE: 44.6545%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 2/90 -> Avg Validation SMAPE: 44.8012%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 3/90 -> Avg Validation SMAPE: 44.6682%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 4/90 -> Avg Validation SMAPE: 44.6206%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 5/90 -> Avg Validation SMAPE: 45.0006%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 6/90 -> Avg Validation SMAPE: 44.8170%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 7/90 -> Avg Validation SMAPE: 45.7036%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 8/90 -> Avg Validation SMAPE: 46.2975%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 9/90 -> Avg Validation SMAPE: 45.0692%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 10/90 -> Avg Validation SMAPE: 44.4838%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 11/90 -> Avg Validation SMAPE: 45.0086%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 12/90 -> Avg Validation SMAPE: 46.4897%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 13/90 -> Avg Validation SMAPE: 45.0529%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 14/90 -> Avg Validation SMAPE: 45.6782%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 15/90 -> Avg Validation SMAPE: 44.8635%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 16/90 -> Avg Validation SMAPE: 45.1102%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 17/90 -> Avg Validation SMAPE: 45.5337%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 18/90 -> Avg Validation SMAPE: 45.4568%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 19/90 -> Avg Validation SMAPE: 44.5231%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 20/90 -> Avg Validation SMAPE: 44.8127%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 21/90 -> Avg Validation SMAPE: 44.5604%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 22/90 -> Avg Validation SMAPE: 44.5169%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 23/90 -> Avg Validation SMAPE: 44.7802%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 24/90 -> Avg Validation SMAPE: 44.7362%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 25/90 -> Avg Validation SMAPE: 44.7535%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 26/90 -> Avg Validation SMAPE: 44.5260%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 27/90 -> Avg Validation SMAPE: 45.2858%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 28/90 -> Avg Validation SMAPE: 44.8579%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 29/90 -> Avg Validation SMAPE: 44.8238%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 30/90 -> Avg Validation SMAPE: 44.5306%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 31/90 -> Avg Validation SMAPE: 44.5645%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 32/90 -> Avg Validation SMAPE: 44.1874%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 33/90 -> Avg Validation SMAPE: 44.4584%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 34/90 -> Avg Validation SMAPE: 44.4387%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 35/90 -> Avg Validation SMAPE: 44.7959%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 36/90 -> Avg Validation SMAPE: 44.4689%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 37/90 -> Avg Validation SMAPE: 44.9810%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 38/90 -> Avg Validation SMAPE: 45.3149%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 39/90 -> Avg Validation SMAPE: 47.9449%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 40/90 -> Avg Validation SMAPE: 44.4154%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 41/90 -> Avg Validation SMAPE: 44.4383%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 42/90 -> Avg Validation SMAPE: 44.1932%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 43/90 -> Avg Validation SMAPE: 43.9819%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 44/90 -> Avg Validation SMAPE: 44.6927%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 45/90 -> Avg Validation SMAPE: 44.5589%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 46/90 -> Avg Validation SMAPE: 44.4298%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 47/90 -> Avg Validation SMAPE: 44.7605%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 48/90 -> Avg Validation SMAPE: 44.4670%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 49/90 -> Avg Validation SMAPE: 44.0109%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 50/90 -> Avg Validation SMAPE: 44.2259%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 51/90 -> Avg Validation SMAPE: 44.0362%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 52/90 -> Avg Validation SMAPE: 44.1873%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 53/90 -> Avg Validation SMAPE: 44.0326%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 54/90 -> Avg Validation SMAPE: 43.9488%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 55/90 -> Avg Validation SMAPE: 43.9420%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 56/90 -> Avg Validation SMAPE: 43.8432%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 57/90 -> Avg Validation SMAPE: 44.3434%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 58/90 -> Avg Validation SMAPE: 43.9786%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 59/90 -> Avg Validation SMAPE: 44.0343%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 60/90 -> Avg Validation SMAPE: 44.5244%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 61/90 -> Avg Validation SMAPE: 44.1237%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 62/90 -> Avg Validation SMAPE: 43.8720%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 63/90 -> Avg Validation SMAPE: 43.9674%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 64/90 -> Avg Validation SMAPE: 44.0141%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 65/90 -> Avg Validation SMAPE: 44.1179%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 66/90 -> Avg Validation SMAPE: 43.9670%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 67/90 -> Avg Validation SMAPE: 43.7591%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 68/90 -> Avg Validation SMAPE: 43.6953%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 69/90 -> Avg Validation SMAPE: 43.8847%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 70/90 -> Avg Validation SMAPE: 43.8004%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 71/90 -> Avg Validation SMAPE: 43.7944%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 72/90 -> Avg Validation SMAPE: 43.8325%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 73/90 -> Avg Validation SMAPE: 43.8545%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 74/90 -> Avg Validation SMAPE: 43.8192%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 75/90 -> Avg Validation SMAPE: 43.8677%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 76/90 -> Avg Validation SMAPE: 43.9249%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 77/90 -> Avg Validation SMAPE: 43.7527%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 78/90 -> Avg Validation SMAPE: 43.8299%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 79/90 -> Avg Validation SMAPE: 43.6824%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 80/90 -> Avg Validation SMAPE: 43.8238%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 81/90 -> Avg Validation SMAPE: 43.7320%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 82/90 -> Avg Validation SMAPE: 43.6962%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 83/90 -> Avg Validation SMAPE: 43.7049%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 84/90 -> Avg Validation SMAPE: 43.7532%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 85/90 -> Avg Validation SMAPE: 43.7616%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 86/90 -> Avg Validation SMAPE: 43.6836%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 87/90 -> Avg Validation SMAPE: 43.7414%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 88/90 -> Avg Validation SMAPE: 43.6678%\r\n",
      "** New best model found! Saving to ./saved_models/best_model.pth **\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 89/90 -> Avg Validation SMAPE: 43.6811%\r\n",
      "--------------------------------------------------\r\n",
      "Epoch 90/90 -> Avg Validation SMAPE: 43.6826%\r\n",
      "--------------------------------------------------\r\n",
      "\r\n",
      "============================================================\r\n",
      "Training finished. Starting prediction...\r\n",
      "============================================================\r\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\r\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "Predicting:   0%|                                       | 0/293 [00:00<?, ?it/s]/kaggle/working/ddp_price_prediction_final_stable.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with autocast():\r\n",
      "Predicting: 100%|█████████████████████████████| 293/293 [02:07<00:00,  2.30it/s]\r\n",
      "\r\n",
      "Predictions saved to output.csv\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ddp_price_prediction_final_stable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5721b9",
   "metadata": {
    "papermill": {
     "duration": 1.083184,
     "end_time": "2025-10-13T17:22:59.489406",
     "exception": false,
     "start_time": "2025-10-13T17:22:58.406222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8470549,
     "sourceId": 13355010,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8456677,
     "sourceId": 13369123,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12988.175581,
   "end_time": "2025-10-13T17:23:00.806472",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-13T13:46:32.630891",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
