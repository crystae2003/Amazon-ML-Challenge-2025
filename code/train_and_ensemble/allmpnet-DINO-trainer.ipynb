{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13355010,"sourceType":"datasetVersion","datasetId":8470549},{"sourceId":13366961,"sourceType":"datasetVersion","datasetId":8456677}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.30.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:53:46.173375Z","iopub.execute_input":"2025-10-13T11:53:46.173594Z","iopub.status.idle":"2025-10-13T11:53:59.398601Z","shell.execute_reply.started":"2025-10-13T11:53:46.173570Z","shell.execute_reply":"2025-10-13T11:53:59.397693Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.30.2\n  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.32.5)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.30.2) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.30.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.30.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.30.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.30.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.30.2) (2024.2.0)\nDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, huggingface-hub, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3 tokenizers-0.13.3 transformers-4.30.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T11:56:15.212393Z","iopub.execute_input":"2025-10-13T11:56:15.213267Z","iopub.status.idle":"2025-10-13T11:57:35.938168Z","shell.execute_reply.started":"2025-10-13T11:56:15.213236Z","shell.execute_reply":"2025-10-13T11:57:35.937480Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\nCollecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.35.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nDownloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.6/486.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.2\n    Uninstalling transformers-4.30.2:\n      Successfully uninstalled transformers-4.30.2\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.1.1 tokenizers-0.22.1 transformers-4.57.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile ddp_price_prediction_final_stable.py\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n# 🔄 CHANGED: Removed Bert specific imports, added SentenceTransformer and kept the scheduler\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport os\nimport pickle\n\n# DDP Imports\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\n# Mixed Precision Imports\nfrom torch.cuda.amp import GradScaler, autocast\n\n# ============== 1. CONFIGURATION ==============\n# 🔄 CHANGED: Updated the model name\nMODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\nMAX_LENGTH = 64\nBATCH_SIZE = 128\nEPOCHS = 50\nLEARNING_RATE = 2e-5\nSAVE_DIR = './saved_models'\nEMBEDDINGS_PATH = '/kaggle/input/embeddings/embeddings/embeddings_images.pkl'\nID_TO_IGNORE = '279285'\nPRETRAINED_MODEL_PATH = '/kaggle/working/saved_models/best_model.pth'\nCHECKPOINT_PATH = os.path.join(SAVE_DIR, 'best_model.pth')\n\n\n# ============== 2. DDP SETUP ==============\ndef ddp_setup():\n    dist.init_process_group(backend=\"nccl\")\n    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n\n# ============== 3. PREPROCESSING ==============\ndef extract_value(text):\n    match = re.search(r\"Value:\\s*(\\d+\\.?\\d*)\", str(text))\n    return float(match.group(1)) if match else 0.0\n\ndef load_embeddings(path):\n    if not os.path.exists(path): return {}\n    try:\n        with open(path, 'rb') as f: return pickle.load(f)\n    except Exception as e:\n        print(f\"Error loading embeddings file: {e}.\")\n        return {}\n        \n# ============== 4. PYTORCH DATASET ==============\nclass ProductTextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, embeddings_dict, is_test=False):\n        self.df = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.embeddings_dict = embeddings_dict\n        self.is_test = is_test\n        self.numeric_cols = ['extracted_value']\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text, sample_id = str(row['catalog_content']), str(row['sample_id'])\n        inputs = self.tokenizer.encode_plus(\n            text, add_special_tokens=True, max_length=self.max_length,\n            padding='max_length', truncation=True, return_tensors='pt'\n        )\n        numeric_values = row[self.numeric_cols].values.astype(np.float32)\n        numeric_features = torch.tensor(numeric_values, dtype=torch.float32)\n        \n        embedding_data = self.embeddings_dict.get(sample_id, torch.ones(768, dtype=torch.float32))\n        if not isinstance(embedding_data, torch.Tensor):\n            image_embedding = torch.tensor(embedding_data, dtype=torch.float32)\n        else:\n            image_embedding = embedding_data\n            \n        item = {\n            'input_ids': inputs['input_ids'].squeeze(0), 'attention_mask': inputs['attention_mask'].squeeze(0),\n            'numeric_features': numeric_features, 'image_embedding': image_embedding\n        }\n        if self.is_test:\n            item['sample_id'] = row['sample_id']\n        else:\n            item['price'] = torch.tensor(row['price'], dtype=torch.float32)\n        return item\n\n# ============== 5. MODEL ARCHITECTURE ==============\n# 🔄 CHANGED: Updated the model architecture to use SentenceTransformer\nclass TextPlusNumericModel(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(TextPlusNumericModel, self).__init__()\n        # Use SentenceTransformer instead of BertModel\n        self.sbert = SentenceTransformer(MODEL_NAME)\n        # Get embedding dimension dynamically\n        text_embedding_dim = self.sbert.get_sentence_embedding_dimension()\n        image_embedding_dim = 768 # Assuming image embedding dim is fixed\n        numeric_dim = 1\n        \n        combined_size = text_embedding_dim + numeric_dim + image_embedding_dim\n        \n        self.regression_head = nn.Sequential(\n            nn.Linear(combined_size, 1024), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, numeric_features, image_embedding):\n        # The forward pass for SentenceTransformer requires a dictionary\n        token_features = {'input_ids': input_ids, 'attention_mask': attention_mask}\n        sbert_output = self.sbert(token_features)\n        text_features = sbert_output['sentence_embedding']\n        \n        combined_features = torch.cat([text_features, numeric_features, image_embedding], dim=1)\n        price = self.regression_head(combined_features)\n        return price\n\ndef smape_loss(y_pred, y_true, epsilon=1e-8):\n    numerator = torch.abs(y_pred - y_true)\n    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n    return torch.mean(numerator / (denominator + epsilon)) * 100\n\n# ============== 6. MAIN TRAINING FUNCTION ==============\ndef main_training_and_prediction():\n    ddp_setup()\n    rank = int(os.environ[\"LOCAL_RANK\"])\n    device = torch.device(f\"cuda:{rank}\")\n    best_val_smape = float('inf')\n    \n    if rank == 0:\n        print(\"=\"*60 + \"\\nStarting Training...\\n\" + \"=\"*60)\n        os.makedirs(SAVE_DIR, exist_ok=True)\n\n    train_df = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/splits/splits/train.csv')\n    val_df = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/splits/splits/val.csv')\n    train_df = train_df[train_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n    val_df = val_df[val_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n    if rank == 0: print(f\"Removed sample ID '{ID_TO_IGNORE}'.\")\n\n    scaler = MinMaxScaler()\n    train_df['extracted_value'] = train_df['catalog_content'].apply(extract_value)\n    val_df['extracted_value'] = val_df['catalog_content'].apply(extract_value)\n    scaler.fit(train_df[['extracted_value']])\n    train_df['extracted_value'] = scaler.transform(train_df[['extracted_value']])\n    val_df['extracted_value'] = scaler.transform(val_df[['extracted_value']])\n    train_df['price'] = np.log1p(train_df['price'])\n    val_df['price'] = np.log1p(val_df['price'])\n\n    embeddings_dict = load_embeddings(EMBEDDINGS_PATH)\n    \n    # 🔄 CHANGED: Initialize model first to get access to its tokenizer\n    # We need to create the model on the correct device before wrapping with DDP\n    model = TextPlusNumericModel().to(device)\n    tokenizer = model.sbert.tokenizer\n\n    train_dataset = ProductTextDataset(train_df, tokenizer, MAX_LENGTH, embeddings_dict)\n    val_dataset = ProductTextDataset(val_df, tokenizer, MAX_LENGTH, embeddings_dict)\n    train_sampler, val_sampler = DistributedSampler(train_dataset), DistributedSampler(val_dataset, shuffle=False)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n\n    if PRETRAINED_MODEL_PATH and os.path.exists(PRETRAINED_MODEL_PATH):\n        if rank == 0: print(f\"--- Loading pretrained model from: {PRETRAINED_MODEL_PATH} ---\")\n        model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))\n    else:\n        if rank == 0: print(\"--- Starting from scratch. ---\")\n\n    model = DDP(model, device_ids=[rank], find_unused_parameters=True)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    \n    scaler = GradScaler()\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(0.1 * len(train_loader) * EPOCHS), \n        num_training_steps=len(train_loader) * EPOCHS\n    )\n\n    for epoch in range(EPOCHS):\n        train_sampler.set_epoch(epoch)\n        model.train()\n        train_loop = tqdm(train_loader, leave=False, disable=(rank != 0))\n        if rank == 0: train_loop.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n\n        for batch in train_loop:\n            input_ids, attention_mask, numerics, img_embeds, prices = (\n                batch['input_ids'].to(device), batch['attention_mask'].to(device),\n                batch['numeric_features'].to(device), batch['image_embedding'].to(device),\n                batch['price'].to(device)\n            )\n            optimizer.zero_grad(set_to_none=True)\n\n            with autocast():\n                outputs = model(input_ids, attention_mask, numerics, img_embeds)\n                loss = loss_fn(outputs.squeeze(), prices)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            if rank == 0: train_loop.set_postfix(loss=f\"{loss.item():.4f}\")\n        \n        model.eval()\n        total_val_smape = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                with autocast():\n                    log_prices_pred = model(\n                        input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device),\n                        numeric_features=batch['numeric_features'].to(device), image_embedding=batch['image_embedding'].to(device)\n                    )\n                prices_pred_orig = torch.expm1(log_prices_pred.squeeze())\n                prices_true_orig = torch.expm1(batch['price'].to(device))\n                total_val_smape += smape_loss(prices_pred_orig, prices_true_orig).item()\n        \n        val_metrics_tensor = torch.tensor([total_val_smape, len(val_loader)]).to(device)\n        dist.all_reduce(val_metrics_tensor, op=dist.ReduceOp.SUM)\n\n        if rank == 0:\n            avg_val_smape = val_metrics_tensor[0] / val_metrics_tensor[1]\n            print(f\"Epoch {epoch+1}/{EPOCHS} -> Avg Validation SMAPE: {avg_val_smape:.4f}%\")\n            if avg_val_smape < best_val_smape:\n                best_val_smape = avg_val_smape\n                print(f\"** New best model found! Saving to {CHECKPOINT_PATH} **\")\n                torch.save(model.module.state_dict(), CHECKPOINT_PATH)\n            print(\"-\" * 50)\n            \n    dist.destroy_process_group()\n\n# ============== 7. LAUNCHER & PREDICTION ==============\nif __name__ == \"__main__\":\n    main_training_and_prediction()\n\n    if int(os.environ.get(\"RANK\", \"0\")) == 0:\n        print(\"\\n\" + \"=\"*60 + \"\\nTraining finished. Starting prediction...\\n\" + \"=\"*60)\n\n        if os.path.exists(CHECKPOINT_PATH):\n            device = torch.device(\"cuda:0\")\n            final_model = TextPlusNumericModel()\n            final_model.load_state_dict(torch.load(CHECKPOINT_PATH))\n            final_model.to(device)\n            final_model.eval()\n\n            test_df = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/dataset/dataset/test.csv')\n            train_df_for_scaler = pd.read_csv('/kaggle/input/amazon-ml-dataset-csv/splits/splits/train.csv')\n            test_df = test_df[test_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n            \n            scaler = MinMaxScaler()\n            train_df_for_scaler['extracted_value'] = train_df_for_scaler['catalog_content'].apply(extract_value)\n            test_df['extracted_value'] = test_df['catalog_content'].apply(extract_value)\n            scaler.fit(train_df_for_scaler[['extracted_value']])\n            test_df['extracted_value'] = scaler.transform(test_df[['extracted_value']])\n            \n            # 🔄 CHANGED: Get tokenizer from the SentenceTransformer model\n            tokenizer = final_model.sbert.tokenizer\n            embeddings_dict = load_embeddings(EMBEDDINGS_PATH)\n            test_dataset = ProductTextDataset(test_df, tokenizer, MAX_LENGTH, embeddings_dict, is_test=True)\n            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2)\n\n            all_predictions, all_sample_ids = [], []\n            with torch.no_grad():\n                for batch in tqdm(test_loader, desc=\"Predicting\"):\n                    with autocast():\n                        outputs = final_model(\n                            input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device), \n                            numeric_features=batch['numeric_features'].to(device), image_embedding=batch['image_embedding'].to(device)\n                        )\n                    preds = np.expm1(outputs.squeeze().cpu().numpy())\n                    all_predictions.extend(preds if preds.ndim > 0 else [preds])\n                    all_sample_ids.extend(batch['sample_id'])\n\n            submission_df = pd.DataFrame({'sample_id': all_sample_ids, 'price': all_predictions})\n            submission_df.to_csv('output.csv', index=False)\n            print(\"\\nPredictions saved to output.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:43:21.823209Z","iopub.execute_input":"2025-10-13T14:43:21.823785Z","iopub.status.idle":"2025-10-13T14:43:21.834457Z","shell.execute_reply.started":"2025-10-13T14:43:21.823760Z","shell.execute_reply":"2025-10-13T14:43:21.833714Z"}},"outputs":[{"name":"stdout","text":"Overwriting ddp_price_prediction_final_stable.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 ddp_price_prediction_final_stable.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:43:22.995384Z","iopub.execute_input":"2025-10-13T14:43:22.996079Z","iopub.status.idle":"2025-10-13T16:40:12.394367Z","shell.execute_reply.started":"2025-10-13T14:43:22.996056Z","shell.execute_reply":"2025-10-13T16:40:12.393351Z"}},"outputs":[{"name":"stdout","text":"W1013 14:43:24.852000 3471 torch/distributed/run.py:792] \nW1013 14:43:24.852000 3471 torch/distributed/run.py:792] *****************************************\nW1013 14:43:24.852000 3471 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1013 14:43:24.852000 3471 torch/distributed/run.py:792] *****************************************\n2025-10-13 14:43:30.642597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-10-13 14:43:30.642615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760366610.664753    3473 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760366610.664873    3474 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760366610.671560    3473 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1760366610.671812    3474 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n============================================================\nStarting Training...\n============================================================\nRemoved sample ID '279285'.\n--- Loading pretrained model from: /kaggle/working/saved_models/best_model.pth ---\n/kaggle/working/ddp_price_prediction_final_stable.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/kaggle/working/ddp_price_prediction_final_stable.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1/50:   0%|                                       | 0/250 [00:00<?, ?it/s]/kaggle/working/ddp_price_prediction_final_stable.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/kaggle/working/ddp_price_prediction_final_stable.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/kaggle/working/ddp_price_prediction_final_stable.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/kaggle/working/ddp_price_prediction_final_stable.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/50 -> Avg Validation SMAPE: 46.4606%\n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 2/50 -> Avg Validation SMAPE: 46.6883%                                    \n--------------------------------------------------\nEpoch 3/50 -> Avg Validation SMAPE: 46.9072%                                    \n--------------------------------------------------\nEpoch 4/50 -> Avg Validation SMAPE: 46.4545%                                    \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 8/50 -> Avg Validation SMAPE: 46.7827%                                    \n--------------------------------------------------\nEpoch 9/50 -> Avg Validation SMAPE: 46.6546%                                    \n--------------------------------------------------\nEpoch 10/50 -> Avg Validation SMAPE: 46.3506%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 11/50 -> Avg Validation SMAPE: 46.3450%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 12/50 -> Avg Validation SMAPE: 47.3324%                                   \n--------------------------------------------------\nEpoch 13/50 -> Avg Validation SMAPE: 47.0132%                                   \n--------------------------------------------------\nEpoch 14/50 -> Avg Validation SMAPE: 46.2204%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 15/50 -> Avg Validation SMAPE: 46.1151%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 16/50 -> Avg Validation SMAPE: 46.4646%                                   \n--------------------------------------------------\nEpoch 17/50 -> Avg Validation SMAPE: 45.6545%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 18/50 -> Avg Validation SMAPE: 46.7158%                                   \n--------------------------------------------------\nEpoch 19/50 -> Avg Validation SMAPE: 45.8119%                                   \n--------------------------------------------------\nEpoch 20/50 -> Avg Validation SMAPE: 45.8744%                                   \n--------------------------------------------------\nEpoch 21/50 -> Avg Validation SMAPE: 45.8699%                                   \n--------------------------------------------------\nEpoch 22/50 -> Avg Validation SMAPE: 46.7344%                                   \n--------------------------------------------------\nEpoch 23/50 -> Avg Validation SMAPE: 45.8373%                                   \n--------------------------------------------------\nEpoch 24/50 -> Avg Validation SMAPE: 45.7253%                                   \n--------------------------------------------------\nEpoch 25/50 -> Avg Validation SMAPE: 46.0091%                                   \n--------------------------------------------------\nEpoch 26/50 -> Avg Validation SMAPE: 45.7124%                                   \n--------------------------------------------------\nEpoch 27/50 -> Avg Validation SMAPE: 45.6371%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 28/50 -> Avg Validation SMAPE: 45.6695%                                   \n--------------------------------------------------\nEpoch 29/50 -> Avg Validation SMAPE: 45.5608%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 30/50 -> Avg Validation SMAPE: 45.4507%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 31/50 -> Avg Validation SMAPE: 45.6090%                                   \n--------------------------------------------------\nEpoch 32/50 -> Avg Validation SMAPE: 47.1455%                                   \n--------------------------------------------------\nEpoch 33/50 -> Avg Validation SMAPE: 45.3762%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 34/50 -> Avg Validation SMAPE: 45.1482%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 35/50 -> Avg Validation SMAPE: 45.4147%                                   \n--------------------------------------------------\nEpoch 36/50 -> Avg Validation SMAPE: 45.3210%                                   \n--------------------------------------------------\nEpoch 37/50 -> Avg Validation SMAPE: 45.1882%                                   \n--------------------------------------------------\nEpoch 38/50 -> Avg Validation SMAPE: 45.1954%                                   \n--------------------------------------------------\nEpoch 39/50 -> Avg Validation SMAPE: 45.1716%                                   \n--------------------------------------------------\nEpoch 40/50 -> Avg Validation SMAPE: 45.1100%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 41/50 -> Avg Validation SMAPE: 44.8565%                                   \n** New best model found! Saving to ./saved_models/best_model.pth **\n--------------------------------------------------\nEpoch 42/50 -> Avg Validation SMAPE: 44.8847%                                   \n--------------------------------------------------\nEpoch 43/50 -> Avg Validation SMAPE: 44.9637%                                   \n--------------------------------------------------\nEpoch 44/50 -> Avg Validation SMAPE: 45.0589%                                   \n--------------------------------------------------\nEpoch 45/50 -> Avg Validation SMAPE: 45.0397%                                   \n--------------------------------------------------\nEpoch 46/50 -> Avg Validation SMAPE: 44.9793%                                   \n--------------------------------------------------\nEpoch 47/50 -> Avg Validation SMAPE: 45.0295%                                   \n--------------------------------------------------\nEpoch 48/50 -> Avg Validation SMAPE: 45.0238%                                   \n--------------------------------------------------\nEpoch 49/50 -> Avg Validation SMAPE: 44.9941%                                   \n--------------------------------------------------\nEpoch 50/50 -> Avg Validation SMAPE: 44.9850%                                   \n--------------------------------------------------\n\n============================================================\nTraining finished. Starting prediction...\n============================================================\nPredicting:   0%|                                       | 0/293 [00:00<?, ?it/s]/kaggle/working/ddp_price_prediction_final_stable.py:270: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nPredicting: 100%|█████████████████████████████| 293/293 [01:14<00:00,  3.94it/s]\n\nPredictions saved to output.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}