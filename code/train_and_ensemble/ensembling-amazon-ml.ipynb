{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13355010,"sourceType":"datasetVersion","datasetId":8470549},{"sourceId":13369123,"sourceType":"datasetVersion","datasetId":8456677},{"sourceId":13371340,"sourceType":"datasetVersion","datasetId":8482926}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predict_sbert.py\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport os\nimport re\nimport pickle\n\n# Configs\nMODEL_NAME_SBERT = 'sentence-transformers/all-mpnet-base-v2'\nCHECKPOINT_PATH_SBERT = '/kaggle/input/model-weights-amazon-ml/best_model_all_mp_net_ashish.pth'\nTEST_CSV_PATH = '/kaggle/input/amazon-ml-dataset-csv/preprocessed/val_split_final.csv'\nTRAIN_CSV_PATH = '/kaggle/input/amazon-ml-dataset-csv/preprocessed/train_split_final.csv'\nEMBEDDINGS_PATH = '/kaggle/input/embeddings/embeddings/embeddings_images.pkl'\nID_TO_IGNORE = '279285'\nMAX_LENGTH = 64\nBATCH_SIZE = 128\n\ndef extract_value(text):\n    match = re.search(r\"Value:\\s*(\\d+\\.?\\d*)\", str(text))\n    return float(match.group(1)) if match else 0.0\n\ndef load_embeddings(path):\n    if not os.path.exists(path):\n        print(\"Image Embeddings path doesn't exist\")\n        return {}\n    try:\n        with open(path, 'rb') as f:\n            print(\"Image Embeddings Loading\")\n            return pickle.load(f)\n    except Exception as e:\n        print(f\"Error loading embeddings file: {e}.\")\n        return {}\n\nclass ProductTextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, embeddings_dict):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_length\n        self.embeddings = embeddings_dict\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        text = str(row.catalog_content)\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        image_embedding = self.embeddings.get(str(row.sample_id))\n        \n        if image_embedding is None:\n            raise ValueError(f\"Missing image embedding for sample_id: {row.sample_id}\")\n                \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'numeric_features': torch.tensor([row.extracted_value], dtype=torch.float),\n            'image_embedding': torch.tensor(image_embedding, dtype=torch.float),\n            'sample_id': row.sample_id\n        }\n\nclass SBERTModel(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(SBERTModel, self).__init__()\n        self.sbert = SentenceTransformer(MODEL_NAME_SBERT)\n        text_embedding_dim = self.sbert.get_sentence_embedding_dimension()\n        combined_size = text_embedding_dim + 1 + 768\n        self.regression_head = nn.Sequential(\n            nn.Linear(combined_size, 1024), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, numeric_features, image_embedding):\n        output = self.sbert({'input_ids': input_ids, 'attention_mask': attention_mask})\n        text_features = output['sentence_embedding']\n        combined = torch.cat([text_features, numeric_features, image_embedding], dim=1)\n        return self.regression_head(combined)\n\ndef predict(model, dataloader, device):\n    model.to(device)\n    model.eval()\n    all_preds, all_ids = [], []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"SBERT Predicting\"):\n            outputs = model(\n                input_ids=batch['input_ids'].to(device),\n                attention_mask=batch['attention_mask'].to(device),\n                numeric_features=batch['numeric_features'].to(device),\n                image_embedding=batch['image_embedding'].to(device)\n            )\n            preds = np.expm1(outputs.squeeze().cpu().numpy())\n            all_preds.extend(preds if preds.ndim > 0 else [preds])\n            all_ids.extend(batch['sample_id'].cpu().numpy())\n    return pd.DataFrame({'sample_id': all_ids, 'price': all_preds})\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    test_df = pd.read_csv(TEST_CSV_PATH)\n    train_df = pd.read_csv(TRAIN_CSV_PATH)\n    test_df = test_df[test_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n\n    scaler = MinMaxScaler()\n    train_df['extracted_value'] = train_df['catalog_content'].apply(extract_value)\n    test_df['extracted_value'] = test_df['catalog_content'].apply(extract_value)\n    scaler.fit(train_df[['extracted_value']])\n    test_df['extracted_value'] = scaler.transform(test_df[['extracted_value']])\n\n    embeddings = load_embeddings(EMBEDDINGS_PATH)\n\n    if embeddings:\n        print(\"Embeddings loaded successfully.\")\n        print(f\"Number of embeddings loaded: {len(embeddings)}\")\n    else:\n        print(\"Embeddings failed to load or are empty.\")\n\n    \n    model = SBERTModel()\n    model.load_state_dict(torch.load(CHECKPOINT_PATH_SBERT, map_location=device))\n    tokenizer = model.sbert.tokenizer\n    dataset = ProductTextDataset(test_df, tokenizer, MAX_LENGTH, embeddings)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n    preds_df = predict(model, loader, device)\n    preds_df.to_csv(\"output_sbert.csv\", index=False)\n    print(\"Saved: output_sbert.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.30.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predict_bert.py\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertModel, BertTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport os\nimport re\nimport pickle\n\n\n# Configs\nMODEL_NAME_BERT = 'bert-base-uncased'\nCHECKPOINT_PATH_BERT = '/kaggle/input/model-weights-amazon-ml/best_model_44_48.pth'\nTEST_CSV_PATH = '/kaggle/input/amazon-ml-dataset-csv/preprocessed/val_split_final.csv'\nTRAIN_CSV_PATH = '/kaggle/input/amazon-ml-dataset-csv/preprocessed/train_split_final.csv'\nEMBEDDINGS_PATH = '/kaggle/input/embeddings/embeddings/embeddings_images.pkl'\nID_TO_IGNORE = -1\nMAX_LENGTH = 64\nBATCH_SIZE = 128\n\ndef extract_value(text):\n    match = re.search(r\"Value:\\s*(\\d+\\.?\\d*)\", str(text))\n    return float(match.group(1)) if match else 0.0\n\ndef load_embeddings(path):\n    if not os.path.exists(path): return {}\n    try:\n        with open(path, 'rb') as f: return pickle.load(f)\n    except Exception as e:\n        print(f\"Error loading embeddings file: {e}.\")\n        return {}\n\nclass ProductTextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, embeddings_dict):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_length\n        self.embeddings = embeddings_dict\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        text = str(row.catalog_content)\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        image_embedding = self.embeddings.get(str(row.sample_id))\n        \n        if image_embedding is None:\n            raise ValueError(f\"Missing image embedding for sample_id: {row.sample_id}\")\n                \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'numeric_features': torch.tensor([row.extracted_value], dtype=torch.float),\n            'image_embedding': torch.tensor(image_embedding, dtype=torch.float),\n            'sample_id': row.sample_id\n        }\n\nclass BERTModel(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(BERTModel, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME_BERT)\n        combined_size = 768 + 1 + 768\n        self.regression_head = nn.Sequential(\n            nn.Linear(combined_size, 1024), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, numeric_features, image_embedding):\n        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        combined = torch.cat([text_features, numeric_features, image_embedding], dim=1)\n        return self.regression_head(combined)\n\ndef predict(model, dataloader, device):\n    model.to(device)\n    model.eval()\n    all_preds, all_ids = [], []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"BERT Predicting\"):\n            outputs = model(\n                input_ids=batch['input_ids'].to(device),\n                attention_mask=batch['attention_mask'].to(device),\n                numeric_features=batch['numeric_features'].to(device),\n                image_embedding=batch['image_embedding'].to(device)\n            )\n            preds = np.expm1(outputs.squeeze().cpu().numpy())\n            all_preds.extend(preds if preds.ndim > 0 else [preds])\n            all_ids.extend(batch['sample_id'].cpu().numpy())\n    return pd.DataFrame({'sample_id': all_ids, 'price': all_preds})\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    test_df = pd.read_csv(TEST_CSV_PATH)\n    train_df = pd.read_csv(TRAIN_CSV_PATH)\n    test_df = test_df[test_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n\n    scaler = MinMaxScaler()\n    train_df['extracted_value'] = train_df['catalog_content'].apply(extract_value)\n    test_df['extracted_value'] = test_df['catalog_content'].apply(extract_value)\n    scaler.fit(train_df[['extracted_value']])\n    test_df['extracted_value'] = scaler.transform(test_df[['extracted_value']])\n\n    embeddings = load_embeddings(EMBEDDINGS_PATH)\n\n    model = BERTModel()\n    model.load_state_dict(torch.load(CHECKPOINT_PATH_BERT, map_location=device))\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_BERT)\n    dataset = ProductTextDataset(test_df, tokenizer, MAX_LENGTH, embeddings)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n    preds_df = predict(model, loader, device)\n    preds_df.to_csv(\"output_bert.csv\", index=False)\n    print(\"Saved: output_bert.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:51:42.148891Z","iopub.execute_input":"2025-10-13T16:51:42.149344Z","iopub.status.idle":"2025-10-13T16:52:13.255815Z","shell.execute_reply.started":"2025-10-13T16:51:42.149322Z","shell.execute_reply":"2025-10-13T16:52:13.254815Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nBERT Predicting: 100%|██████████| 88/88 [00:22<00:00,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"Saved: output_bert.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ensemble_predictions.py\nimport pandas as pd\n\n# Load model predictions\nsbert_df = pd.read_csv(\"output_sbert.csv\")  # Contains sample_id, price\nbert_df = pd.read_csv(\"output_bert.csv\")    # Contains sample_id, price\n\n# Rename to avoid conflicts\nsbert_df.rename(columns={'price': 'price_sbert'}, inplace=True)\nbert_df.rename(columns={'price': 'price_bert'}, inplace=True)\n\n# Merge on sample_id\nmerged_df = pd.merge(sbert_df, bert_df, on=\"sample_id\")\n\n# Simple average (or replace with weighted average if needed)\nmerged_df['price'] = (merged_df['price_sbert'] + merged_df['price_bert']) / 2\n\n# Save final predictions\nmerged_df[['sample_id', 'price']].to_csv(\"output_ensemble.csv\", index=False)\nprint(\"Saved ensemble predictions to output_ensemble.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:52:22.869158Z","iopub.execute_input":"2025-10-13T16:52:22.869463Z","iopub.status.idle":"2025-10-13T16:52:22.908862Z","shell.execute_reply.started":"2025-10-13T16:52:22.869437Z","shell.execute_reply":"2025-10-13T16:52:22.908053Z"}},"outputs":[{"name":"stdout","text":"Saved ensemble predictions to output_ensemble.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# --- Configuration ---\nVAL_FILE = '/kaggle/input/amazon-ml-dataset-csv/splits/splits/val.csv'\nVAL_PRED_FILE = '/kaggle/working/output_ensemble.csv'\nSEED = 42\n\ndef calculate_smape(actual_prices, predicted_prices):\n    \"\"\"\n    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE) as a percentage.\n    \n    Formula: SMAPE = (1/n) * SUM(|P - A| / ((|A| + |P|)/2)) * 100%\n    \"\"\"\n    \n    # Absolute difference between predicted and actual\n    numerator = np.abs(predicted_prices - actual_prices)\n    \n    # Denominator: average of absolute actual and absolute predicted prices\n    denominator = (np.abs(actual_prices) + np.abs(predicted_prices)) / 2\n    \n    # Handle division by zero case: if both prices are zero, the error is 0.\n    # We replace any zero in the denominator with 1 to prevent division by zero, \n    # but the numerator will also be zero, so the fraction remains 0.\n    # This is a common practice for handling zero values in SMAPE.\n    smape_term = np.divide(\n        numerator, \n        denominator, \n        out=np.zeros_like(numerator), # Output array initialized to zeros\n        where=denominator != 0        # Only perform division where denominator is not zero\n    )\n    \n    # Calculate the mean and multiply by 100 for the percentage result\n    smape_percentage = np.mean(smape_term) * 100\n    \n    return smape_percentage\n\n# --- Main execution block ---\nif __name__ == \"__main__\":\n    \n    try:\n        # 2. Load the data\n        df_val = pd.read_csv(VAL_FILE, usecols=['sample_id', 'price'])\n        df_pred = pd.read_csv(VAL_PRED_FILE)\n\n        # Rename the price column in val.csv to 'actual_price' for clarity\n        df_val = df_val.rename(columns={'price': 'actual_price'})\n        \n        # Rename the price column in val-pred.csv to 'predicted_price' for clarity\n        df_pred = df_pred.rename(columns={'price': 'predicted_price'})\n        \n    except FileNotFoundError as e:\n        print(f\"Error: One or both files not found. Please ensure '{VAL_FILE}' and '{VAL_PRED_FILE}' are in the same directory.\")\n        print(e)\n        exit()\n\n    # 3. Merge the two DataFrames on the 'sample_id'\n    # This ensures that each prediction is matched with its correct actual value.\n    merged_df = pd.merge(df_val, df_pred, on='sample_id', how='inner')\n\n    if merged_df.empty:\n        print(\"Error: The merged DataFrame is empty. Check if 'sample_id' values match between the two files.\")\n        exit()\n        \n    print(f\"\\nSuccessfully loaded and merged {len(merged_df)} samples.\")\n    \n    # 4. Extract price vectors\n    actual_prices = merged_df['actual_price'].values\n    predicted_prices = merged_df['predicted_price'].values\n\n    # 5. Calculate SMAPE\n    smape_score = calculate_smape(actual_prices, predicted_prices)\n    \n    # 6. Output the result\n    print(\"-\" * 40)\n    print(f\"SMAPE Score: {smape_score:.4f}%\")\n    print(f\"Interpretation: On average, the predictions are off by {smape_score:.2f}% relative to the average of the actual and predicted prices.\")\n    print(\"-\" * 40)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:52:44.802798Z","iopub.execute_input":"2025-10-13T16:52:44.803518Z","iopub.status.idle":"2025-10-13T16:52:44.905355Z","shell.execute_reply.started":"2025-10-13T16:52:44.803495Z","shell.execute_reply":"2025-10-13T16:52:44.904623Z"}},"outputs":[{"name":"stdout","text":"\nSuccessfully loaded and merged 11250 samples.\n----------------------------------------\nSMAPE Score: 43.5951%\nInterpretation: On average, the predictions are off by 43.60% relative to the average of the actual and predicted prices.\n----------------------------------------\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 3. Predict with BERT model\nprint(\"\\n\" + \"=\"*60)\nprint(\"Starting prediction with BERT model...\")\nif not os.path.exists(CHECKPOINT_PATH_BERT):\n    raise FileNotFoundError(f\"BERT checkpoint not found at: {CHECKPOINT_PATH_BERT}\")\n\nbert_model = BERTModel()\nbert_model.load_state_dict(torch.load(CHECKPOINT_PATH_BERT, map_location=device))\nbert_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_BERT)\n\nbert_test_dataset = ProductTextDataset(test_df, bert_tokenizer, MAX_LENGTH, embeddings_dict, is_test=True)\nbert_test_loader = DataLoader(bert_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\npreds_bert_df = predict(bert_model, bert_test_loader, device)\npreds_bert_df.rename(columns={'price': 'price_bert'}, inplace=True)\npreds_bert_df.to_csv('output_bert.csv', index=False)\nprint(\"Prediction with BERT model finished.\")\nprint(\"=\"*60)\n\n# 4. Ensemble the predictions\nprint(\"\\nEnsembling predictions...\")\n# Merge predictions from both models on 'sample_id'\nensemble_df = pd.merge(preds_sbert_df, preds_bert_df, on='sample_id')\n\n# Simple averaging ensemble. You can also use weighted averaging.\n# Example weighted average: 0.6 * ensemble_df['price_sbert'] + 0.4 * ensemble_df['price_bert']\nensemble_df['price'] = (ensemble_df['price_sbert'] + ensemble_df['price_bert']) / 2\n\n# Prepare final submission file\nsubmission_df = ensemble_df[['sample_id', 'price']]\n\n# 5. Save the final output\noutput_filename = 'output-ensemble.csv'\nsubmission_df.to_csv(output_filename, index=False)\nprint(f\"\\nEnsemble predictions successfully saved to {output_filename}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertModel, BertTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport os\n# --- OPTIMIZATION: Added for mixed-precision inference ---\nfrom torch.cuda.amp import autocast\n\n# --- Placeholder Setups ---\n# (Please fill these in with your actual values and helper functions)\n\n# 1. Define your model names and checkpoint paths\nMODEL_NAME_SBERT = 'sentence-transformers/all-mpnet-base-v2'\nMODEL_NAME_BERT = 'bert-base-uncased' # Or your specific BERT model\nCHECKPOINT_PATH_SBERT = '/kaggle/input/model-weights-amazon-ml/best_model_all_mp_net_ashish.pth'\nCHECKPOINT_PATH_BERT = '/kaggle/input/model-weights-amazon-ml/best_model_44_48.pth'\n\n# 2. Define data paths and constants\nTEST_CSV_PATH = '/kaggle/input/amazon-ml-dataset-csv/preprocessed/test_split_final.csv'\nTRAIN_CSV_PATH = '/kaggle/input/amazon-ml-dataset-csv/preprocessed/train_split_final.csv'\nEMBEDDINGS_PATH = 'path/to/your/embeddings.pkl' # e.g., image embeddings\nID_TO_IGNORE = -1 # From your original code\nMAX_LENGTH = 128\nBATCH_SIZE = 128\n\n# 3. Define your helper functions (assuming they exist from your code)\ndef extract_value(text):\n    # Placeholder for your function that extracts a numeric value from text\n    # Example implementation:\n    import re\n    numbers = re.findall(r'\\d+\\.?\\d*', str(text))\n    return float(numbers[0]) if numbers else 0.0\n\ndef load_embeddings(path):\n    # Placeholder for your function that loads image embeddings\n    # Should return a dictionary mapping an identifier to an embedding vector\n    # Example: return {'img_id_1': [0.1, 0.2, ...], ...}\n    print(f\"NOTE: Using placeholder for load_embeddings('{path}'). Implement your logic.\")\n    return {}\n\n# 4. Define your custom Dataset class\nclass ProductTextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, embeddings_dict, is_test=False):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_length\n        self.is_test = is_test\n        self.embeddings = embeddings_dict\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        text = str(row.catalog_content)\n        \n        # Check if tokenizer is from SentenceTransformer\n        is_sbert_tokenizer = hasattr(self.tokenizer, 'encode_plus') and not hasattr(self.tokenizer, 'prepare_for_model')\n\n        if is_sbert_tokenizer:\n             # Standard tokenization for models like BERT\n            encoding = self.tokenizer.encode_plus(\n                text,\n                add_special_tokens=True,\n                max_length=self.max_len,\n                return_token_type_ids=False,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                return_tensors='pt',\n            )\n        else: # Assuming Hugging Face Transformers tokenizer\n            encoding = self.tokenizer(\n                text,\n                add_special_tokens=True,\n                max_length=self.max_len,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                return_tensors='pt'\n            )\n\n        item = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'numeric_features': torch.tensor([row.extracted_value], dtype=torch.float),\n            'image_embedding': torch.tensor(self.embeddings.get(row.sample_id, [0]*768), dtype=torch.float), # Fallback to zeros\n            'sample_id': row.sample_id\n        }\n        return item\n\n# --- Model Architectures ---\n\n# Architecture 1 (using all-mpnet-base-v2)\nclass SBERTModel(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(SBERTModel, self).__init__()\n        self.sbert = SentenceTransformer(MODEL_NAME_SBERT)\n        text_embedding_dim = self.sbert.get_sentence_embedding_dimension()\n        image_embedding_dim = 768\n        numeric_dim = 1\n        combined_size = text_embedding_dim + numeric_dim + image_embedding_dim\n        \n        self.regression_head = nn.Sequential(\n            nn.Linear(combined_size, 1024), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, numeric_features, image_embedding):\n        token_features = {'input_ids': input_ids, 'attention_mask': attention_mask}\n        sbert_output = self.sbert(token_features)\n        text_features = sbert_output['sentence_embedding']\n        \n        combined_features = torch.cat([text_features, numeric_features, image_embedding], dim=1)\n        price = self.regression_head(combined_features)\n        return price\n\n# Architecture 2 (using BERT)\nclass BERTModel(nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(BERTModel, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME_BERT)\n        combined_size = 768 + 1 + 768 # bert_output + numeric + image\n        self.regression_head = nn.Sequential(\n            nn.Linear(combined_size, 1024), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(512, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, numeric_features, image_embedding):\n        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        combined_features = torch.cat([text_features, numeric_features, image_embedding], dim=1)\n        price = self.regression_head(combined_features)\n        return price\n\n# --- Prediction Function ---\n\ndef predict(model, data_loader, device):\n    \"\"\"Runs prediction on a given model and dataloader.\"\"\"\n    model.to(device)\n    model.eval()\n    \n    all_predictions = []\n    all_sample_ids = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=f\"Predicting with {model.__class__.__name__}\"):\n            # --- OPTIMIZATION: Use autocast for mixed-precision ---\n            with autocast(enabled=device.type == 'cuda'):\n                outputs = model(\n                    input_ids=batch['input_ids'].to(device),\n                    attention_mask=batch['attention_mask'].to(device),\n                    numeric_features=batch['numeric_features'].to(device),\n                    image_embedding=batch['image_embedding'].to(device)\n                )\n            # Assuming your model outputs log(price + 1), so we use expm1\n            preds = np.expm1(outputs.squeeze().cpu().numpy())\n            all_predictions.extend(preds if preds.ndim > 0 else [preds])\n            all_sample_ids.extend(batch['sample_id'].cpu().numpy())\n            \n    return pd.DataFrame({'sample_id': all_sample_ids, 'price': all_predictions})\n\n\n# --- Main Ensembling Script ---\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # --- OPTIMIZATION: Set num_workers for parallel data loading ---\n    # Use 2 or 4 workers if your system can handle it, 0 for debugging.\n    NUM_WORKERS = 2 \n\n    # 1. Load and preprocess data\n    print(\"Loading and preprocessing test data...\")\n    test_df = pd.read_csv(TEST_CSV_PATH)\n    train_df_for_scaler = pd.read_csv(TRAIN_CSV_PATH)\n    test_df = test_df[test_df['sample_id'] != ID_TO_IGNORE].reset_index(drop=True)\n    \n    scaler = MinMaxScaler()\n    train_df_for_scaler['extracted_value'] = train_df_for_scaler['catalog_content'].apply(extract_value)\n    test_df['extracted_value'] = test_df['catalog_content'].apply(extract_value)\n    scaler.fit(train_df_for_scaler[['extracted_value']])\n    test_df['extracted_value'] = scaler.transform(test_df[['extracted_value']])\n    \n    embeddings_dict = load_embeddings(EMBEDDINGS_PATH)\n\n    # 2. Predict with SBERT model\n    print(\"\\n\" + \"=\"*60)\n    print(\"Starting prediction with SBERT model...\")\n    if not os.path.exists(CHECKPOINT_PATH_SBERT):\n        raise FileNotFoundError(f\"SBERT checkpoint not found at: {CHECKPOINT_PATH_SBERT}\")\n        \n    sbert_model = SBERTModel()\n    sbert_model.load_state_dict(torch.load(CHECKPOINT_PATH_SBERT, map_location=device))\n    sbert_tokenizer = sbert_model.sbert.tokenizer\n    \n    sbert_test_dataset = ProductTextDataset(test_df, sbert_tokenizer, MAX_LENGTH, embeddings_dict, is_test=True)\n    # --- OPTIMIZATION: Added num_workers to DataLoader ---\n    sbert_test_loader = DataLoader(sbert_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n    \n    preds_sbert_df = predict(sbert_model, sbert_test_loader, device)\n    preds_sbert_df.rename(columns={'price': 'price_sbert'}, inplace=True)\n    print(\"Prediction with SBERT model finished.\")\n    print(\"=\"*60)\n\n    # 3. Predict with BERT model\n    print(\"\\n\" + \"=\"*60)\n    print(\"Starting prediction with BERT model...\")\n    if not os.path.exists(CHECKPOINT_PATH_BERT):\n        raise FileNotFoundError(f\"BERT checkpoint not found at: {CHECKPOINT_PATH_BERT}\")\n\n    bert_model = BERTModel()\n    bert_model.load_state_dict(torch.load(CHECKPOINT_PATH_BERT, map_location=device))\n    bert_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_BERT)\n\n    bert_test_dataset = ProductTextDataset(test_df, bert_tokenizer, MAX_LENGTH, embeddings_dict, is_test=True)\n    # --- OPTIMIZATION: Added num_workers to DataLoader ---\n    bert_test_loader = DataLoader(bert_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n    preds_bert_df = predict(bert_model, bert_test_loader, device)\n    preds_bert_df.rename(columns={'price': 'price_bert'}, inplace=True)\n    print(\"Prediction with BERT model finished.\")\n    print(\"=\"*60)\n\n    # 4. Ensemble the predictions\n    print(\"\\nEnsembling predictions...\")\n    # Merge predictions from both models on 'sample_id'\n    ensemble_df = pd.merge(preds_sbert_df, preds_bert_df, on='sample_id')\n    \n    # Simple averaging ensemble. You can also use weighted averaging.\n    # Example weighted average: 0.6 * ensemble_df['price_sbert'] + 0.4 * ensemble_df['price_bert']\n    ensemble_df['price'] = (ensemble_df['price_sbert'] + ensemble_df['price_bert']) / 2\n    \n    # Prepare final submission file\n    submission_df = ensemble_df[['sample_id', 'price']]\n    \n    # 5. Save the final output\n    output_filename = 'output-ensemble.csv'\n    submission_df.to_csv(output_filename, index=False)\n    print(f\"\\nEnsemble predictions successfully saved to {output_filename}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}